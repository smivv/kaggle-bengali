model_params:
  model: EfficientNetMultiHeadNet
  backbone_params:
    model_name: efficientnet-b7
    pretrained: True
    requires_grad: False
    in_channels: 1
    num_classes: 1

  neck_params:
    hiddens: [512]
    layer_fn: {"module": "Linear", "bias": False}
    norm_fn: BatchNorm1d
    activation_fn: ReLU
    dropout_fn: Dropout
    residual: "soft"

  heads_params:
    grapheme_root_pred:
      hiddens: [256, &grapheme_num_classes 168]
      layer_fn: [{"module": Linear, "bias": False},
                 {"module": ArcFaceProduct}]
    vowel_diacritic_pred:
      hiddens: [256, &vowel_num_classes 11]
      layer_fn: [{"module": Linear, "bias": False},
                 {"module": ArcFaceProduct}]
    consonant_diacritic_pred:
      hiddens: [256, &consonant_num_classes 7]
      layer_fn: [{"module": Linear, "bias": False},
                 {"module": ArcFaceProduct}]

args:
  expdir: "./src/"
  logdir: "./logs/efficientnet"

  seed: 69
  verbose: True
  per_gpu_scaling: True

stages:
  data_params:
    dataset_path: "/workspace/Datasets/BENGALI"
    target_to_use: [0, 1, 2]
    image_size: 224
    test_size: 0.2
    test_only: False
    use_parquet: False
    files_to_load: None

    batch_size: 64
    num_workers: 0

  criterion_params:
    _key_value: True
    ce:
      criterion: CrossEntropyLoss
    fl:
      criterion: FocalLossMultiClass
    arc:
      criterion: ArcFaceLoss

  state_params:
    num_epochs: 2
    main_metric: &MAIN_METRIC "_total_accuracy01"
    minimize_metric: False

  scheduler_params:
    scheduler: MultiStepLR
    milestones: [10]
    gamma: 0.3

  callbacks_params:
    grapheme_criterion_callback:
      callback: CriterionCallback
      input_key: &grapheme_input_key "grapheme_root"
      output_key: &grapheme_output_key "grapheme_root_pred"
      criterion_key: "ce"
      prefix: &grapheme_loss_prefix "loss_ce_grapheme_root"
    vowel_criterion_callback:
      callback: CriterionCallback
      input_key: &vowel_input_key "vowel_diacritic"
      output_key: &vowel_output_key "vowel_diacritic_pred"
      criterion_key: "ce"
      prefix: &vowel_loss_prefix "loss_ce_vowel_diacritic"
    consonant_criterion_callback:
      callback: CriterionCallback
      input_key: &consonant_input_key "consonant_diacritic"
      output_key: &consonant_output_key "consonant_diacritic_pred"
      criterion_key: "ce"
      prefix: &consonant_loss_prefix "loss_ce_consonant_diacritic"
    loss_aggregator_callback:
      callback: CriterionAggregatorCallback
      prefix: loss
      loss_aggregate_fn: sum
      loss_keys: [*grapheme_loss_prefix, *vowel_loss_prefix, *consonant_loss_prefix]

    grapheme_accuracy_callback:
      callback: AccuracyCallback
      input_key: *grapheme_input_key
      output_key: *grapheme_output_key
      num_classes: *grapheme_num_classes
      prefix: &grapheme_acc_prefix "acc_grapheme"
      accuracy_args: [1]
    vowel_accuracy_callback:
      callback: AccuracyCallback
      input_key: *vowel_input_key
      output_key: *vowel_output_key
      num_classes: *vowel_num_classes
      prefix: &vowel_acc_prefix "acc_vowel"
      accuracy_args: [1]
    consonant_accuracy_callback:
      callback: AccuracyCallback
      input_key: *consonant_input_key
      output_key: *consonant_output_key
      num_classes: *consonant_num_classes
      prefix: &consonant_acc_prefix "acc_consonant"
      accuracy_args: [1]
    metrics_aggregator_callback:
      callback: MetricAggregatorCallback
      prefix: *MAIN_METRIC
      metric_aggregate_fn: weighted_sum
      metric_keys: {"acc_grapheme01": 0.5, "acc_vowel01": 0.25, "acc_consonant01": 0.25}

    visualization_callback:
      callback: VisualizationCallback
      input_keys: ["image"]
#      optimizer:
#        callback: OptimizerCallback
#      scheduler:
#        callback: SchedulerCallback
#        reduce_metric: *main_metric
#      saver:
#        callback: CheckpointCallback

  # train head
  stage1:
    state_params:
      num_epochs: 8

    optimizer_params:
#      optimizer: Lookahead
#      base_optimizer_params:
      optimizer: Adam
      lr: 0.001
      weight_decay: 0.0001

    scheduler_params:
      scheduler: MultiStepLR
      milestones: [8]
      gamma: 0.3

  # tune whole network
  stage2:
    state_params:
      num_epochs: 24

    optimizer_params:
#      optimizer: Lookahead
#      base_optimizer_params:
      optimizer: Adam
      lr: 0.0003

    scheduler_params:
      scheduler: MultiStepLR
      milestones: [12, 20]
      gamma: 0.3
